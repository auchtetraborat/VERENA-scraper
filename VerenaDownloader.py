import requests
import math
from bs4 import BeautifulSoup
from typing import Tuple, List

class VerenaDownloader:

    BASE_URL = "https://www.schulministerium.nrw.de"
    LANDING_URL_BASE = BASE_URL + "/BiPo/Verena"
    LISTING_URL_BASE = BASE_URL + "/BiPo/Verena/angebote"

    def __init__(self):
        self.session = requests.Session()

    def __scrape_landing_page(self) -> Tuple[int, str, str]:
        """ Returns (openings_count: int, listing_url_part: str, action_id: str)

        Example: (513, "/BiPo/Verena/angebote?action=595.1764087184088", "595.1764087184088")

        Scrapes the VERENA landing page to get a session cookie, matching actionid 
        to access the listing view and the count of job offerings in the listing.
        """
        landing_request = self.session.get(self.LANDING_URL_BASE)
        landing_soup = BeautifulSoup(landing_request.text, 'html.parser')
        links = landing_soup.findAll("a", {"title": "Zu den Stellenausschreibungen"})
        for link in links:
            if "Derzeit im Netz verÃ¶ffentlichte Ausschreibungen:" in link.text:
                openings_count = link.find_next('strong').text
                listing_url_part = link['href']
                access_listing_action_id = listing_url_part.replace("/BiPo/Verena/angebote?action=","")
                return int(openings_count), listing_url_part, access_listing_action_id

    def __scrape_listing_page_initial(self, listing_url: str) -> Tuple[str,str, str]:
        """ Returns (listing url with new actionid, blocksize 100 & valid suchid, suchid, actionid) 

        Example: ("/BiPo/Verena/angebote?action=509.9848906326322&block=b100&suchid=188736", "188736", "509.9848906326322")
    
        Scrapes the VERENA listing page to get a listing url with blocksize = 100 and valid suchid.
        suchid is generated by the backend and stores your search preferences.
        """
        listing_request = self.session.get(listing_url)
        listing_soup = BeautifulSoup(listing_request.text, 'html.parser')
        blocksize_selector = listing_soup.find('div', id="blockauswahl")
        # -1 is blocksize 100, also gets a such_id
        searchid_url_part = blocksize_selector.findAll('a')[-1]['href']
        search_id = searchid_url_part.split("=")[-1]
        search_action_id = searchid_url_part.replace("/BiPo/Verena/angebote?action=","").split("&")[0]
        return searchid_url_part, search_id, search_action_id

    def __set_block_size(self, search_id_url):
        self.session.get(search_id_url)

    def __generate_all_listing_urls(self, action_id, search_id, opening_count) -> List[str]:
        
        """ Based on action_id, search_id and opening_count, generates a list of all listing urls.
        
        Example: https://www.schulministerium.nrw.de/BiPo/Verena/angebote?action=901.7040712715743&seite=a1&suchid=188265

        """
        all_urls = []
        # because block size = 100
        site_count = math.ceil(int(opening_count) / 100)
        for curr_site in range(0, site_count):
            curr_site += 1
            listing_format_string = self.LISTING_URL_BASE + "?action={0}&seite=a{1}&suchid={2}"
            all_urls.append(listing_format_string.format(action_id, curr_site, search_id))
        return all_urls


    def __scrape_actual_listing(self, urls: List[str]):
        """ Returns list of all scraped listing pages
        """
        scraped_pages = []
        for url in urls:
            r = self.session.get(url)
            scraped_pages.append(r.text)
        return scraped_pages

    def scrape(self):
        """ Returns list of sources of all listing pages of the VERENA job listing portal
        """
        opening_count, listing_url_part, access_listing_action_id = self.__scrape_landing_page()
        listing_url = self.BASE_URL + listing_url_part
        searchid_url_part, search_id, search_action_id = self.__scrape_listing_page_initial(listing_url)
        searchid_url = self.BASE_URL + searchid_url_part
        self.__set_block_size(searchid_url)
        all_listing_urls = self.__generate_all_listing_urls(search_action_id, search_id, opening_count)
        return self.__scrape_actual_listing(all_listing_urls)
        
        













